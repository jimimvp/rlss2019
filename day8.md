# Exploration in POMDP

1) Exhaustive Exploration
2) Information seeking exploration

Meta-environment setting

Whitebox approach, asking questions to agent.

Random strategy, task-dependent strategy, info-seeking strategy.

#### My comment
it should not be about tweaking the reward function, this separates reinforcement learning from optimal control 

Naive proxy: local uncertainty, prediction error
Bottleneck states: mazes are interesting.

Proxy, local information gain.
Brownian motion maximizes information gain. 


### Representation Learning

Contrastive approach
